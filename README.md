# ECTransformer : A hierarchical GPU scheduling Framework for Transformer inference in edge computing 

Edge computing has been widely used for deploying and accelerating deep learning applications. Equipped with GPUs, edge nodes can process concurrent incoming inference requests of the deep learning model. However, the existing parallel processing methods for the inference tasks cannot efficiently use the GPU resources. This paper investigates the popular Transformer deep learning model and develops ECTransformer, a parallel inference framework for Transformers that deployed with GPUs in edge computing. In the framework, the Transformer model is partitioned and deployed in usersâ€™ devices and the edge node to efficiently utilize their processing power. The concurrent inference tasks with different sizes are dynamically packaged in a scheduling queue and sent in batch to an encoder-decoder pipeline for processing. ECTransformer can significantly reduce the overheads of parallel processing and the usage of GPU memory. Experiment results show that ECTransformer can save up to 25.3$\%$ of GPU memory usage and improve 6.1 times of processing speed.

